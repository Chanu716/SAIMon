{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4821773e",
   "metadata": {},
   "source": [
    "# SAIMon - Anomaly Detection Experiments\n",
    "\n",
    "This notebook demonstrates various anomaly detection algorithms for time series data.\n",
    "\n",
    "## Contents\n",
    "1. Data Collection from Prometheus\n",
    "2. Data Exploration and Visualization\n",
    "3. Feature Engineering\n",
    "4. Anomaly Detection Algorithms\n",
    "   - Z-Score\n",
    "   - Isolation Forest\n",
    "   - LSTM Autoencoder\n",
    "5. Model Evaluation\n",
    "6. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55237450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Prometheus\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ada625a",
   "metadata": {},
   "source": [
    "## 1. Connect to Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Prometheus\n",
    "PROMETHEUS_URL = 'http://localhost:9090'\n",
    "prom = PrometheusConnect(url=PROMETHEUS_URL, disable_ssl=True)\n",
    "\n",
    "print(f\"Connected to Prometheus at {PROMETHEUS_URL}\")\n",
    "\n",
    "# List available metrics\n",
    "metrics = prom.all_metrics()\n",
    "print(f\"\\nAvailable metrics: {len(metrics)}\")\n",
    "print(\"\\nSample metrics:\")\n",
    "for metric in metrics[:10]:\n",
    "    print(f\"  - {metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d26995",
   "metadata": {},
   "source": [
    "## 2. Fetch Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ddc376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time range\n",
    "end_time = datetime.now()\n",
    "start_time = end_time - timedelta(hours=24)  # Last 24 hours\n",
    "\n",
    "# Choose a metric (change this to your metric)\n",
    "metric_name = 'node_cpu_seconds_total'\n",
    "\n",
    "# Fetch data\n",
    "print(f\"Fetching data for: {metric_name}\")\n",
    "print(f\"Time range: {start_time} to {end_time}\")\n",
    "\n",
    "result = prom.custom_query_range(\n",
    "    query=metric_name,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time,\n",
    "    step='1m'\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "data_points = []\n",
    "for metric_result in result:\n",
    "    labels = metric_result['metric']\n",
    "    values = metric_result['values']\n",
    "    \n",
    "    for timestamp, value in values:\n",
    "        data_points.append({\n",
    "            'timestamp': datetime.fromtimestamp(float(timestamp)),\n",
    "            'value': float(value),\n",
    "            'cpu': labels.get('cpu', 'unknown'),\n",
    "            'instance': labels.get('instance', 'unknown')\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data_points)\n",
    "print(f\"\\n✅ Loaded {len(df)} data points\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d608ad9",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db61f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Data Statistics:\")\n",
    "print(df['value'].describe())\n",
    "\n",
    "# Plot time series\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(df['timestamp'], df['value'], alpha=0.7)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Time Series: {metric_name}')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['value'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Value Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['value'])\n",
    "plt.ylabel('Value')\n",
    "plt.title('Box Plot')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208704f8",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1769d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "df_features = df.copy()\n",
    "\n",
    "# Rolling statistics\n",
    "for window in [5, 10, 30]:\n",
    "    df_features[f'rolling_mean_{window}'] = df_features['value'].rolling(window=window, min_periods=1).mean()\n",
    "    df_features[f'rolling_std_{window}'] = df_features['value'].rolling(window=window, min_periods=1).std().fillna(0)\n",
    "\n",
    "# Time-based features\n",
    "df_features['hour'] = df_features['timestamp'].dt.hour\n",
    "df_features['day_of_week'] = df_features['timestamp'].dt.dayofweek\n",
    "df_features['is_weekend'] = df_features['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"Features created:\")\n",
    "print(df_features.columns.tolist())\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f14c712",
   "metadata": {},
   "source": [
    "## 5. Anomaly Detection - Z-Score Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f6a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-Score anomaly detection\n",
    "threshold = 3.0\n",
    "\n",
    "mean = df_features['value'].mean()\n",
    "std = df_features['value'].std()\n",
    "\n",
    "df_features['z_score'] = np.abs((df_features['value'] - mean) / std)\n",
    "df_features['is_anomaly_zscore'] = df_features['z_score'] > threshold\n",
    "\n",
    "anomalies_zscore = df_features[df_features['is_anomaly_zscore']]\n",
    "print(f\"Z-Score detected {len(anomalies_zscore)} anomalies ({len(anomalies_zscore)/len(df_features)*100:.2f}%)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(df_features['timestamp'], df_features['value'], label='Normal', alpha=0.7)\n",
    "plt.scatter(anomalies_zscore['timestamp'], anomalies_zscore['value'], \n",
    "            color='red', label='Anomaly', s=50, zorder=5)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Z-Score Anomaly Detection (threshold={threshold})')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963d191",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection - Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2729c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for Isolation Forest\n",
    "feature_cols = ['value', 'rolling_mean_10', 'rolling_std_10', 'hour']\n",
    "X = df_features[feature_cols].fillna(0)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Isolation Forest\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.1,  # Expected proportion of anomalies\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Predict (-1 for anomaly, 1 for normal)\n",
    "predictions = iso_forest.fit_predict(X_scaled)\n",
    "df_features['is_anomaly_iforest'] = predictions == -1\n",
    "\n",
    "# Anomaly scores (lower is more anomalous)\n",
    "scores = iso_forest.decision_function(X_scaled)\n",
    "df_features['anomaly_score_iforest'] = 1 - (scores - scores.min()) / (scores.max() - scores.min())\n",
    "\n",
    "anomalies_iforest = df_features[df_features['is_anomaly_iforest']]\n",
    "print(f\"Isolation Forest detected {len(anomalies_iforest)} anomalies ({len(anomalies_iforest)/len(df_features)*100:.2f}%)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(df_features['timestamp'], df_features['value'], label='Normal', alpha=0.7)\n",
    "plt.scatter(anomalies_iforest['timestamp'], anomalies_iforest['value'], \n",
    "            color='red', label='Anomaly', s=50, zorder=5)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Isolation Forest Anomaly Detection')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3afa84c",
   "metadata": {},
   "source": [
    "## 7. Compare Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affcc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare detections\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Z-Score\n",
    "axes[0].plot(df_features['timestamp'], df_features['value'], alpha=0.7)\n",
    "axes[0].scatter(anomalies_zscore['timestamp'], anomalies_zscore['value'], \n",
    "                color='red', s=50, zorder=5)\n",
    "axes[0].set_title(f'Z-Score Method ({len(anomalies_zscore)} anomalies)')\n",
    "axes[0].set_ylabel('Value')\n",
    "\n",
    "# Isolation Forest\n",
    "axes[1].plot(df_features['timestamp'], df_features['value'], alpha=0.7)\n",
    "axes[1].scatter(anomalies_iforest['timestamp'], anomalies_iforest['value'], \n",
    "                color='red', s=50, zorder=5)\n",
    "axes[1].set_title(f'Isolation Forest ({len(anomalies_iforest)} anomalies)')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(f\"Total data points: {len(df_features)}\")\n",
    "print(f\"Z-Score anomalies: {len(anomalies_zscore)} ({len(anomalies_zscore)/len(df_features)*100:.2f}%)\")\n",
    "print(f\"Isolation Forest anomalies: {len(anomalies_iforest)} ({len(anomalies_iforest)/len(df_features)*100:.2f}%)\")\n",
    "\n",
    "# Overlap\n",
    "both_methods = df_features[df_features['is_anomaly_zscore'] & df_features['is_anomaly_iforest']]\n",
    "print(f\"Detected by both methods: {len(both_methods)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75926e31",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec5902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save anomalies to CSV\n",
    "output_file = f'../data/processed/anomalies_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "anomalies_combined = df_features[\n",
    "    df_features['is_anomaly_zscore'] | df_features['is_anomaly_iforest']\n",
    "][['timestamp', 'value', 'is_anomaly_zscore', 'is_anomaly_iforest', 'z_score', 'anomaly_score_iforest']]\n",
    "\n",
    "anomalies_combined.to_csv(output_file, index=False)\n",
    "print(f\"✅ Anomalies exported to: {output_file}\")\n",
    "print(f\"Total anomalies: {len(anomalies_combined)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
